# -*- coding: utf-8 -*-

"""GPTNeo_notebook.ipynb

Automatically generated by Colab.
"""

!git clone https://github.com/EleutherAI/GPTNeo
# %cd GPTNeo
!pip3 install -q -r requirements.txt
pretrained_model = None
dataset = None

from google.colab import auth
auth.authenticate_user()
!gcloud init

path_to_cloud_bucket = 'gs://your-cloud-bucket/' #@param {type:"string"}

## Set Up Dataset



# Select a Dataset:
import os
dataset = 'Sampling_Only' #@param ["Sampling_Only", "OpenWebText", "YoutubeSubtitles", "HackerNews", "NIHExporter", "Custom"]

if dataset == "Sampling_Only":
  pass
elif dataset == 'OpenWebText':
  !wget https://the-eye.eu/public/AI/pile_preliminary_components/openwebtext2.jsonl.zst.tar -O openwebtext.tar.xz
  !tar xf openwebtext.tar.xz
  dataset_path = "openwebtext"
  dataset_name = dataset_path
  out_name = dataset_name + "_tokenized"
elif dataset == 'YoutubeSubtitles':
  os.makedirs('data', exist_ok=True)
  !wget https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst -O data/yt_subs.jsonl.zst
  dataset_path = 'data'
  dataset_name = 'ytsubs'
  out_name = dataset_name + "_tokenized"
elif dataset == 'HackerNews':
  os.makedirs('data', exist_ok=True)
  !wget https://the-eye.eu/public/AI/pile_preliminary_components/hn.tar.gz -O data/hn.tar.gz
  dataset_path = 'data'
  dataset_name = 'hackernews'
  out_name = dataset_name + "_tokenized"
elif dataset == "NIHExporter":
  os.makedirs('data', exist_ok=True)
  !wget https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst -O data/NIH_ExPORTER_awarded_grant_text.jsonl.zst
  dataset_path = 'data'
  os.system('mv NIH_ExPORTER_awarded_grant_text.jsonl.zst ./data')
  dataset_name = 'nihexporter'
  out_name = dataset_name + "_tokenized"
elif dataset == "Custom":
  dataset_path = input('Enter the path to the folder containing your data: ')
  dataset_name = input('Enter the name of your dataset: ')
  out_name = dataset_name + "_tokenized"
else:
  raise NotImplementedError('please select from available options: ["OpenWebText", "YoutubeSubtitles", "HackerNews", "NIHExporter", "Custom"]')

### Tokenize and Upload Data


# Tokenize Data
!python data/create_tfrecords.py --input_dir /content/GPTNeo/$dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1

# copy the data to your bucket
if not path_to_cloud_bucket.endswith('/'):
  path_to_cloud_bucket += '/'
copy_loc = path_to_cloud_bucket + "datasets/" + dataset
!gsutil -m cp -r /content/GPTNeo/$out_name $copy_loc
!gsutil ls $path_to_cloud_bucket



# Commented out IPython magic to ensure Python compatibility.
# %%writefile configs/dataset_configs/Sampling_Only.json
# 
# {
#   "path": "gs://eleutherai/datasets/Sampling_Only/Sampling_Only*.tfrecords",
#   "eval_path": "",
#   "n_vocab": 50256,
#   "tokenizer_is_pretrained": true,
#   "tokenizer_path": "gpt2",
#   "eos_id": 50256,
#   "padding_id": 50257
# }
#

## Set Model Configs



# Commented out IPython magic to ensure Python compatibility.
# %%writefile configs/GPT3_XL.json
# 
# {
#     "n_head": 16,
#     "n_vocab": 50257,
#     "embed_dropout": 0,
#     "lr": 0.0002,
#     "lr_decay": "cosine",
#     "warmup_steps": 3000,
#     "beta1": 0.9,
#     "beta2": 0.95,
#     "epsilon": 1e-8,
#     "opt_name": "adam",
#     "weight_decay": 0,
#     "train_batch_size": 256,
#     "attn_dropout": 0,
#     "train_steps": 600000,
#     "eval_steps": 0,
#     "predict_steps": 1,
#     "res_dropout": 0,
#     "eval_batch_size": 4,
#     "predict_batch_size": 1,
#     "iterations": 100,
#     "n_embd": 2048,
#     "datasets": [["pile", null, null, null]],
#     "model": "GPT",
#     "model_path": "gs://eleutherai/GPT3_XL",
#     "n_ctx": 2048,
#     "n_layer": 24,
#     "scale_by_depth": true,
#     "scale_by_in": false,
#     "attention_types" :  [[["global", "local"],12]],
#     "mesh_shape": "x:4,y:2",
#     "layout": "intermediate_expanded:x,heads:x,vocab:n_vocab,memory_length:y,embd:y",
#     "activation_function": "gelu",
#     "recompute_grad": true,
#     "gradient_clipping": 1.0,
#     "tokens_per_mb_per_replica": 2048,
#     "precision": "bfloat16"
# }


## Training from Scratch

!python3 main.py --model colab_XL --steps_per_checkpoint 500 --tpu colab

## Pretrained Model


# @title Download pretrained model weights:
pretrained_model = 'GPT3_2-7B' #@param ["GPT3_XL", "GPT3_2-7B"]
!wget -m -np -c -U "eye02" -w 2 -R "index.html*" "https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/"
path_to_local_weights = f"/content/GPTNeo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}"



# upload to your bucket
bucket_base = "gs://" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]
!gsutil -m cp -r $path_to_local_weights $bucket_base


!gsutil ls $bucket_base


# @title Modify config for colab.

import json
from pprint import pprint

path_to_model = "" #@param {type:"string"}
batch_size = 8 #@param {type:"integer"}
dset = ""  #@param {type:"string"}
mesh_shape = "x:4,y:2" #@param {type:"string"}
train_steps = 1000 #@param {type:"integer"}
steps_per_checkpoint = 500 #@param {type:"integer"}
start_step = 400000 if pretrained_model == "GPT3_2-7B" else 362000

if path_to_model == "":
  path_to_model = f'{bucket_base.strip("/")}/{pretrained_model}'
print(f'MODEL PATH: {path_to_model}\n')

if dset == "" and dataset != "Sampling_Only":
  dset = dataset
elif dataset is None and dset == "":
  dset = "pile"

def pad_to_multiple_of(n, mult):
  """
  pads n to a multiple of mult
  """
  extra = n % mult
  if extra > 0:
      n = n + mult - extra
  return n

with open(f'{path_to_local_weights}/config.json', 'r') as f:
  data = json.load(f)
  pprint(data)
  dset_val = [[dset, None, None, None]] if dset != "" else data["datasets"]
  mods = {
          "mesh_shape": mesh_shape,
          "layout": "intermediate_expanded:x,heads:x,memory_length:y,embd:y",
          "model_path": path_to_model,
          "datasets": dset_val,
          "train_steps": start_step + train_steps,
          "eval_steps": 0,
          "train_batch_size": batch_size,
          "predict_batch_size": batch_size
        }
  data.update(mods)
  print('\n--->\n')
  pprint(data)
  with open(f'configs/{pretrained_model}.json', 'w') as outfile:
    json.dump(data, outfile, indent=2)

### Begin Fine-Tuning



!python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab

### Sample from your model


# Commented out IPython magic to ensure Python compatibility.
# %%writefile example_prompt.txt
# 
# class GPT(nn.Module):
#     """  the full GPT language model, with a context size of block_size """
# 
#     def __init__(self, config):
#         super().__init__()
# 
#         # input embedding stem
#         self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)
#         self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))
#         self.drop = nn.Dropout(config.embd_pdrop)
#         # transformer
#         self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
#         # decoder head
#         self.ln_f = nn.LayerNorm(config.n_embd)
#         self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
# 
#         self.block_size = config.block_size
#         self.apply(self._init_weights)
# 
#         logger.info("number of parameters: %e", sum(p.numel() for p in self.parameters()))

!python3 main.py --model $pretrained_model --steps_per_checkpoint 500 --tpu colab --predict --prompt example_prompt.txt

"""# Evaluating the model

This section assumes you are using a pretrained model and relies on variables created in the `Pretrained model` section.

## Wikitext

Download the wikitext test set:
"""

wikitext103_src = "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
!wget $wikitext103_src
!unzip wikitext-103-raw-v1.zip

"""Tokenize and upload to bucket:

"""

!mkdir wikitext
!mv /content/GPTNeo/wikitext-103-raw/wiki.test.raw wikitext/wikitext_test.txt

# Tokenize Data
!python data/create_tfrecords.py --input_dir wikitext --name wikitext --files_per 1000 --output_dir wikitext_tokenized --write_dataset_config --processes 1 --wikitext-detokenize

# copy the data to your bucket
if not path_to_cloud_bucket.endswith('/'):
  path_to_cloud_bucket += '/'
copy_loc = path_to_cloud_bucket
!gsutil -m cp -r wikitext_tokenized $copy_loc
!gsutil ls $path_to_cloud_bucket

"""Now make a dataset config that points to the tokenized wikitext data:"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile configs/dataset_configs/wikitext.json
# 
# {
#   "path": "",
#   "eval_path": "gs://test-bucket-neo/wikitext_tokenized/*.tfrecords",
#   "n_vocab": 50256,
#   "tokenizer_is_pretrained": true,
#   "tokenizer_path": "gpt2",
#   "eos_id": 50256,
#   "padding_id": 50257
# }
#

"""And update your model config to point to that dataset:

"""

# @title Modify config for wikitext.

import json
from pprint import pprint

batch_size = 8 #@param {type:"integer"}
assert pretrained_model is not None
with open(f'configs/{pretrained_model}.json', 'r') as f:
  data = json.load(f)
  pprint(data)
  dset_val = [["wikitext", None, None, None]]
  mods = {
          "datasets": dset_val,
          "eval_steps": 139 // batch_size,
          "train_batch_size": batch_size,
          "eval_batch_size": batch_size,
        }
  data.update(mods)
  print('\n--->\n')
  pprint(data)
  with open(f'configs/{pretrained_model}.json', 'w') as outfile:
    json.dump(data, outfile, indent=2)

"""Now run model in eval mode over tokenized data:"""

!python3 main.py --eval --tpu colab --model $pretrained_model

"""## Lambada
"""

# @title Modify config for Lambada.

import json
from pprint import pprint

batch_size = 8 #@param {type:"integer"}
assert pretrained_model is not None
with open(f'configs/{pretrained_model}.json', 'r') as f:
  data = json.load(f)
  mods = {
          "datasets": dset_val,
          "eval_steps": 0,
          "train_batch_size": batch_size,
          "eval_batch_size": batch_size,
          "eval_tasks": ["lambada"]
        }
  data.update(mods)
  print('\n--->\n')
  pprint(data)
  with open(f'configs/{pretrained_model}.json', 'w') as outfile:
    json.dump(data, outfile, indent=2)

"""Now run the eval:"""

!python3 main.py --eval --tpu colab --model $pretrained_model