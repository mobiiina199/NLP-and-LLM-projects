# -*- coding: utf-8 -*-
"""Masked language modeling and image classification.ipynb

Automatically generated by Colab.

## Set-up environment

In this notebook, we are going to show that you can do masked language modeling and image classification with the same architecture. We first install HuggingFace Transformers.
"""



## Using Perceiver on text


from transformers import PerceiverTokenizer, PerceiverForMaskedLM

tokenizer = PerceiverTokenizer.from_pretrained("deepmind/language-perceiver")
model = PerceiverForMaskedLM.from_pretrained("deepmind/language-perceiver")


import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)



text = "This is an incomplete sentence where some words are missing."
# prepare input
encoding = tokenizer(text, padding="max_length", return_tensors="pt")
# mask " missing.".
encoding.input_ids[0, 52:61] = tokenizer.mask_token_id
inputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)

print("Inputs:", tokenizer.decode(inputs.squeeze()))



# forward pass
outputs = model(inputs=inputs, attention_mask=input_mask)
logits = outputs.logits
masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)
print(tokenizer.decode(masked_tokens_predictions))

"""## Using Perceiver on images

The Perceiver also works really well on images. Here we load our familiar cats image.
"""

from PIL import Image
import requests

url = "https://storage.googleapis.com/perceiver_io/dalmation.jpg"
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image



from transformers import PerceiverFeatureExtractor, PerceiverForImageClassificationLearned

del model
feature_extractor = PerceiverFeatureExtractor.from_pretrained("deepmind/vision-perceiver-learned")
model = PerceiverForImageClassificationLearned.from_pretrained("deepmind/vision-perceiver-learned")

model.to(device)

"""Here we prepare the image for the model, and forward it through it."""

# prepare input
encoding = feature_extractor(image, return_tensors="pt")
inputs, input_mask = encoding.pixel_values.to(device), None
# forward pass
outputs = model(inputs, input_mask)
logits = outputs.logits

"""The model outputs logits of shape (batch_size, num_labels), which in this case will be (1, 1000) - as the model was trained on ImageNet-1k, which includes 1,000 possible classes."""

print("Predicted class:", model.config.id2label[logits.argmax(-1).item()])

"""## Using Perceiver on images (Fourier)

The second variant adds fixed Fourier 2D position embeddings.
"""

from transformers import PerceiverForImageClassificationFourier

del model
model = PerceiverForImageClassificationFourier.from_pretrained("deepmind/vision-perceiver-fourier")
model.to(device)

# prepare input
encoding = feature_extractor(image, return_tensors="pt")
inputs, input_mask = encoding.pixel_values.to(device), None
# forward pass
outputs = model(inputs, input_mask)
logits = outputs.logits

print("Predicted class:", model.config.id2label[logits.argmax(-1).item()])

"""## Using Perceiver on images (Conv)

The third variant applies a Conv2D + maxpool preprocessing operation on the image, before using it for cross-attention with the latents of the Perceiver encoder.
"""

from transformers import PerceiverForImageClassificationConvProcessing

del model
model = PerceiverForImageClassificationConvProcessing.from_pretrained("deepmind/vision-perceiver-conv")
model.to(device)

# prepare input
encoding = feature_extractor(image, return_tensors="pt")
inputs, input_mask = encoding.pixel_values.to(device), None
# forward pass
outputs = model(inputs, input_mask)
logits = outputs.logits

print("Predicted class:", model.config.id2label[logits.argmax(-1).item()])

